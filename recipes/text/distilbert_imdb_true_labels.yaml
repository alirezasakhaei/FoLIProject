# IMDb: DistilBERT with true labels
# This configuration uses the IMDb dataset for sentiment analysis with true labels

# Experiment identification
experiment_id: distilbert_imdb_true_labels_v2

# Model configuration
model:
  model_name: distilbert-base-uncased
  num_classes: 2  # Binary classification: positive/negative
  task_type: text_classification

# Data configuration
data:
  dataset: imdb
  data_root: ./data
  batch_size: 16
  num_workers: 4
  pin_memory: true
  randomization: none  # Use true labels
  max_length: 512  # Maximum sequence length for tokenization
  data_fraction: 1.0  # Use 100% of the data

# Training hyperparameters
training:
  num_epochs: 3  # Following HuggingFace tutorial (2-3 epochs typical for fine-tuning)
  learning_rate: 2e-5  # Following HuggingFace tutorial
  warmup_steps: 500
  optimizer: adamw
  weight_decay: 0.01

# Early stopping
early_stopping:
  early_stopping_enabled: true
  early_stopping_min_epochs: 1
  early_stopping_window: 3

# Logging and checkpointing
logging:
  use_wandb: true
  wandb_project: FOLI-Project
  wandb_entity: alirezasakhaeirad
  save_dir: ./checkpoints/imdb
  log_interval: 100
  eval_strategy: steps  # Automatically set to evaluate 10 times per epoch
  save_strategy: epoch

# Device
device: cuda

# Reproducibility
seed: 42
